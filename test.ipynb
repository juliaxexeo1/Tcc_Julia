{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sktime.datasets import load_from_tsfile_to_dataframe\n",
    "from sktime.datatypes._panel._convert import from_nested_to_2d_array\n",
    "from sklearn.model_selection import cross_validate, RepeatedKFold\n",
    "import lib_all_data_analysis_v3 as lda\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = r'C:/Users/jujuv/OneDrive/Documentos/2020.2/Otimização/wale_xexeo/mit-bih-arrhythmia-database-1.0.0 (1)/mit-bih-arrhythmia-database-1.0.0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSTANTS\n",
    "\n",
    "# List of Patients\n",
    "patients = ['100','101','102','103','104','105','106','107',\n",
    "           '108','109','111','112','113','114','115','116',\n",
    "           '117','118','119','121','122','123','124','200',\n",
    "           '201','202','203','205','207','208','209','210',\n",
    "           '212','213','214','215','217','219','220','221',\n",
    "           '222','223','228','230','231','232','233','234']\n",
    "\n",
    "\n",
    "# Non Beat Symbols\n",
    "nonbeat = ['[','!',']','x','(',')','p','t','u','`',\n",
    "           '\\'','^','|','~','+','s','T','*','D','=','\"','@','Q','?']\n",
    "\n",
    "# Abnormal Beat Symbols\n",
    "abnormal = ['L','R','V','/','A','f','F','j','a','E','J','e','S']\n",
    "\n",
    "# Normal Beat Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_df = pd.DataFrame()\n",
    "\n",
    "for pts in patients:\n",
    "    # Generating filepath for all .atr file names\n",
    "    file = data + pts\n",
    "    # Saving annotation object\n",
    "    annotation = wfdb.rdann(file, 'atr')\n",
    "    # Extracting symbols from the object\n",
    "    sym = annotation.symbol\n",
    "    # Saving value counts\n",
    "    values, counts = np.unique(sym, return_counts=True)\n",
    "    # Writing data points into dataframe\n",
    "    df_sub = pd.DataFrame({'symbol':values, 'Counts':counts, 'Patient Number':[pts]*len(counts)})\n",
    "    # Concatenating all data points  \n",
    "    symbols_df = pd.concat([symbols_df, df_sub],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifying normal, abnormal or nonbeat\n",
    "symbols_df['category'] = -1\n",
    "symbols_df.loc[symbols_df.symbol == 'N','category'] = 0\n",
    "symbols_df.loc[symbols_df.symbol.isin(abnormal), 'category'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "-1     3186\n",
       " 0    75052\n",
       " 1    34409\n",
       "Name: Counts, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols_df.groupby('category').Counts.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ecg(file):    \n",
    "    record = wfdb.rdrecord(file)\n",
    "    annotation = wfdb.rdann(file, 'atr')\n",
    "    \n",
    "    p_signal = record.p_signal\n",
    "\n",
    "    atr_sym = annotation.symbol\n",
    "    atr_sample = annotation.sample\n",
    "    \n",
    "    return p_signal, atr_sym, atr_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ 3\n",
      "J 50\n",
      "N 2700\n",
      "V 3\n",
      "~ 8\n"
     ]
    }
   ],
   "source": [
    "# Analysing annotations value counts for a single record\n",
    "values, counts = np.unique(sym, return_counts=True)\n",
    "for v,c in zip(values, counts):\n",
    "    print(v,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(pts, num_sec, fs, abnormal):\n",
    "    # function for making dataset ignoring non-beats\n",
    "    # input:\n",
    "    #   pts - list of patients\n",
    "    #   num_sec = number of seconds to include before and after the beat\n",
    "    #   fs = frequency\n",
    "    # output: \n",
    "    #   X_all = signal (nbeats , num_sec * fs columns)\n",
    "    #   Y_all = binary is abnormal (nbeats, 1)\n",
    "    #   sym_all = beat annotation symbol (nbeats,1)\n",
    "    \n",
    "    # initialize numpy arrays\n",
    "    num_cols = 2*num_sec * fs\n",
    "    X_all = np.zeros((1,num_cols))\n",
    "    Y_all = np.zeros((1,1))\n",
    "    sym_all = []\n",
    "    \n",
    "    # list to keep track of number of beats across patients\n",
    "    max_rows = []\n",
    "    \n",
    "    for pt in pts:\n",
    "        file = data + pt\n",
    "        \n",
    "        p_signal, atr_sym, atr_sample = load_ecg(file)\n",
    "        \n",
    "        # grab the first signal\n",
    "        p_signal = p_signal[:,0]\n",
    "        \n",
    "        # make df to exclude the nonbeats\n",
    "        df_ann = pd.DataFrame({'atr_sym':atr_sym,\n",
    "                              'atr_sample':atr_sample})\n",
    "        df_ann = df_ann.loc[df_ann.atr_sym.isin(abnormal + ['N'])]\n",
    "        \n",
    "        X,Y,sym = build_XY(p_signal,df_ann, num_cols, abnormal)\n",
    "        sym_all = sym_all+sym\n",
    "        max_rows.append(X.shape[0])\n",
    "        X_all = np.append(X_all,X,axis = 0)\n",
    "        Y_all = np.append(Y_all,Y,axis = 0)\n",
    "        \n",
    "    # drop the first zero row\n",
    "    X_all = X_all[1:,:]\n",
    "    Y_all = Y_all[1:,:]\n",
    "\n",
    "    return X_all, Y_all, sym_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_XY(p_signal, df_ann, num_cols, abnormal):\n",
    "    # this function builds the X,Y matrices for each beat\n",
    "    # it also returns the original symbols for Y\n",
    "    \n",
    "    num_rows = len(df_ann)\n",
    "\n",
    "    X = np.zeros((num_rows, num_cols))\n",
    "    Y = np.zeros((num_rows,1))\n",
    "    sym = []\n",
    "    \n",
    "    # keep track of rows\n",
    "    max_row = 0\n",
    "\n",
    "    for atr_sample, atr_sym in zip(df_ann.atr_sample.values,df_ann.atr_sym.values):\n",
    "\n",
    "        left = max([0,(atr_sample - num_sec*fs) ])\n",
    "        right = min([len(p_signal),(atr_sample + num_sec*fs) ])\n",
    "        x = p_signal[left: right]\n",
    "        if len(x) == num_cols:\n",
    "            X[max_row,:] = x\n",
    "            Y[max_row,:] = int(atr_sym in abnormal)\n",
    "            sym.append(atr_sym)\n",
    "            max_row += 1\n",
    "    X = X[:max_row,:]\n",
    "    Y = Y[:max_row,:]\n",
    "    return X,Y,sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sec = 3\n",
    "fs = 360\n",
    "num_runs = 10\n",
    "cross_val = 5\n",
    "num_kernels = 10000\n",
    "_results = np.zeros(num_runs)\n",
    "dataset_name = ['ECG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all,Y_all, sym_all = make_dataset(patients, num_sec, fs, abnormal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================RUNNING=====================================\n",
      "--------------------------------['earthquakes']---------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Mount dataset results\n",
    "results = pd.DataFrame(index = dataset_name,\n",
    "                       columns = [\"accuracy_mean\",\n",
    "                                  \"accuracy_standard_deviation\",\n",
    "                                  \"time_training_seconds\",\n",
    "                                  \"time_test_seconds\"],\n",
    "                       data = 0,\n",
    "                       dtype='float')\n",
    "results.index.name = \"datasets\"\n",
    "\n",
    "#--- run experiment in Dataset---------------------------------------------------------\n",
    "print(f\"RUNNING\".center(80, \"=\"))\n",
    "\n",
    "#for dataset_name in dataset_names:\n",
    "print(f\"{dataset_name}\".center(80, \"-\"))\n",
    "_timings = np.zeros([5, num_runs])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m val_tex, val_index \u001b[38;5;129;01min\u001b[39;00m kf\u001b[38;5;241m.\u001b[39msplit(\u001b[43mX_all\u001b[49m):\n\u001b[0;32m      3\u001b[0m     X_train, X_test \u001b[38;5;241m=\u001b[39m X_all[val_tex], X_all[val_index]\n\u001b[0;32m      4\u001b[0m     y_train, y_test \u001b[38;5;241m=\u001b[39m Y_all[val_tex], Y_all[val_index]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_all' is not defined"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for val_tex, val_index in kf.split(X_all):\n",
    "    X_train, X_test = X_all[val_tex], X_all[val_index]\n",
    "    y_train, y_test = Y_all[val_tex], Y_all[val_index]\n",
    "    y_train=y_train.astype(int)\n",
    "    y_test=y_test.astype(int)\n",
    "\n",
    "    # -- transform training ------------------------------------------------\n",
    "    time_a = time.perf_counter()\n",
    "    X_train_transform = lda.wave_layer_ts_v2(X_train, 100,['bior3.1', 'bior1.5', 'db11'], 0)\n",
    "    time_b = time.perf_counter()\n",
    "    _timings[0, i] = time_b - time_a\n",
    "    print('terminei 1')\n",
    "\n",
    "    # -- transform test ----------------------------------------------------\n",
    "    time_a = time.perf_counter()\n",
    "    #WALE-a v1 (after tunning)\n",
    "    X_test_transform = lda.wave_layer_ts_v2(X_test, 100,['bior3.1', 'bior1.5', 'db11'], 0)\n",
    "    time_b = time.perf_counter()\n",
    "    _timings[1, i] = time_b - time_a\n",
    "    print('terminei 2')\n",
    "    \n",
    "     # -- training ----------------------------------------------------------\n",
    "    time_a = time.perf_counter()\n",
    "    classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "    classifier.fit(X_train_transform, y_train)\n",
    "    time_b = time.perf_counter()\n",
    "    _timings[2, i] = time_b - time_a\n",
    "    print('terminei 3')\n",
    "\n",
    "    # -- test --------------------------------------------------------------\n",
    "    time_a = time.perf_counter()\n",
    "    _results[i] = classifier.score(X_test_transform, y_test)\n",
    "    time_b = time.perf_counter()\n",
    "    \n",
    "    _timings[3, i] = time_b - time_a\n",
    "    i +=1\n",
    "    print('terminei 4')\n",
    "\n",
    "    print(\"Done.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[2568   58]\n",
      " [ 154 1769]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test_transform)\n",
    "cm = confusion_matrix(y_test, y_pred)  # Compute the confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -- store results ---------------------------------------------------------\n",
    "results.loc[dataset_name, \"accuracy_mean\"] = _results.mean()\n",
    "results.loc[dataset_name, \"accuracy_standard_deviation\"] = _results.std()\n",
    "results.loc[dataset_name, \"time_training_seconds\"] = _timings.mean(1)[[0, 2]].sum()\n",
    "results.loc[dataset_name, \"time_test_seconds\"] = _timings.mean(1)[[1, 3]].sum() #P, R, MATRIZ DE CONFUSÃO\n",
    "#ACURACIA Não é um dado bom\n",
    "#antes de aprender tem que descrever a base.\n",
    "#EQUILIBRAR  A BASE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_mean</th>\n",
       "      <th>accuracy_standard_deviation</th>\n",
       "      <th>time_training_seconds</th>\n",
       "      <th>time_test_seconds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datasets</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>earthquakes</th>\n",
       "      <td>0.951967</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>519.07272</td>\n",
       "      <td>519.889057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             accuracy_mean  accuracy_standard_deviation  \\\n",
       "datasets                                                  \n",
       "earthquakes       0.951967                     0.001429   \n",
       "\n",
       "             time_training_seconds  time_test_seconds  \n",
       "datasets                                               \n",
       "earthquakes              519.07272         519.889057  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6181d1b41fcc57165698c952864db1a8c3ad4b2a29597f8fe6ff7a56a9faf387"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
